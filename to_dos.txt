- folder -> dir
- fix musicxml midi zero division errors
- update complexities, include key/tempo... changes
- when decoding tokens, set instruments to piano for both hands: 
    midi = tokenizer.decode(token_sequence)
    for instrument in midi.instruments:
        instrument.program = 0  # Program 0 = Acoustic Grand Piano
- maybe make all conversion function init parameters arguments of music pipeline function in pipeline.py
- test model and adjust weights 
- exception catchers for new conversion functions
- combine jsons into jsonl -> dataset object before training (write function)
- documentation
- overwork training
- completely overwork dataset preparation, padding with miditok built in classes
- overwork when .vocab vs when _vocab_base is used
- upload tokeniser and model to huggingface hub / maybe together
- also think about the hash
- centralise all hardcoding like file naming conventions like metadata file naming
- use data augmentation from miditok
- implement checker in mytokeniser loading that checks that the params config is valid for the class
- check hash of to_dict for false negatives
- think about max_tok_seq_length for dataset
- maybe save whole tokeniser.to_dict() in model
- build if - else to only save tokeniser again if new model was created
- create constants file to store all formats and constants
- think about what to do with save_with_hash method for tokeniser

INFO:
    - save_pretrained is like save only with the option to push to hub
    - from_pretrained is like initialising via params, but with option to download from hub
    - params is the full tokeniser file containing config, tokenisations, vocab (if pretrained),...