- fix musicxml midi zero division errors
- update complexities, include key/tempo... changes
- when decoding tokens, set instruments to piano for both hands: 
    midi = tokenizer.decode(token_sequence)
    for instrument in midi.instruments:
        instrument.program = 0  # Program 0 = Acoustic Grand Piano
- test model and adjust weights 
- documentation
- upload tokeniser and model to huggingface hub / maybe together # at the end of project maybe and then give links in readme

- save tokenseqs that a model was trained on in database and filter out at dataset creation
- maybe make loading of MyModel and MyTokeniser bulletproof
- do what audiveris says on its website to enhance transcription quality
- for generate classmethod catch non existent saved model 
- use data augmentation from miditok !!! transpose to every key and get *15 data scale
- write generic clean_up function that takes extention and removes all nonfitting files
-think about this: Slide forward by 1 page (i.e., page 1+2, then 2+3, then 3+4...)
- centralise -100 as label padding in constants
- fix that we can also train with untrained tokeniser
- find cleaner solution for prefixing bos token before generation
- maybe remove unecessary stuff from token files again
- adapt exercise splitting to other end lines too (like, final-repeat mix line)
- (generally fix:) put length of "[input:....]" computation of how many lines to clear
- maybe adapt infrastructure to accept single objects OR list List[object] like for conversionoutcomes for logging
- find solution for repeat end splitting: maybe plit at end repeats only if next bar has both keysig and timesig or other beginning bar identifiers (good heuristic)
- keysig_8 error

INFO:
    - training errors with some assertion src < scr... are likely due to wrong max_positions_embedding in model config, or due to token outside of vocab 

    - rn the musicxml splitting option splits not only at every end bar lines but also at repeat ends

    - save_pretrained is like save only with the option to push to hub
    - from_pretrained is like initialising via params, but with option to download from hub
    - params is the full tokeniser file containing config, tokenisations, vocab (if pretrained),...

    3 ways of loading/creating a MyTokeniser:
        - pass in config object in initialiser, this is only for new models with only config
            -> enforced that its a special config of MyTokeniserConfig class
        - pass in tokeniser file in params in initialiser
            -> enforced that when the file is unpacked the classes match
        - call from_pretrained (this will actually go use the 2nd option on the classname it finds in the tokeniser file)
            -> enforced that this returnes an object of MyTokeniser

    2 ways of loading/creating a MyModel:
        - pass in config object in initialiser
        - call from_pretrained (this will call the 1st option)
            -> enforced that architecture (if given) is MyModel, else also requires a tokeniser_hash attr in config 

    pdf preprocessing splits pdfs into groups of pages,
    tradeoff: larger group -> higher likelihood of containing a text containing page that fails group conversion to musicxml, but also makes it less likely that an exercise that goes across multiple pages is split 

    => all input files must contain only exercises with end bar lines at the end, otherwise they will be merged with the following exercise which corrupts the data
        